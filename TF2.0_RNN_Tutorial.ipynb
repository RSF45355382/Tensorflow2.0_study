{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = keras.layers.SimpleRNNCell(3)  # 这里的3代表的这个单元的输出的在维度上是3(前向传递3次)\n",
    "cell.build(input_shape = (None,4)) # 4是输入的的feature宽度\n",
    "\n",
    "\n",
    "# RNN在时间序列逐渐增加的情况下, 会出现梯度爆炸或者梯度弥散"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'kernel:0' shape=(4, 3) dtype=float32, numpy=\n",
       " array([[-0.89848304, -0.8368476 , -0.1992647 ],\n",
       "        [-0.09867746, -0.11240393, -0.26739293],\n",
       "        [-0.80526453,  0.72694147, -0.46086255],\n",
       "        [ 0.21163678, -0.3832984 ,  0.41287196]], dtype=float32)>,\n",
       " <tf.Variable 'recurrent_kernel:0' shape=(3, 3) dtype=float32, numpy=\n",
       " array([[ 0.17403531,  0.98466516, -0.01210808],\n",
       "        [ 0.5858792 , -0.11341876, -0.80242246],\n",
       "        [-0.7914906 ,  0.13255578, -0.5966336 ]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell.trainable_variables\n",
    "# kernel:0 是Wxh, 是与输入进行计算的矩阵\n",
    "# recurrent_kernel:0 是Whh  是与原状态进行计算的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟一个简单的RNN\n",
    "simp_cell = keras.layers.SimpleRNNCell(64)\n",
    "simp_cell.build(input_shape = (None,100))\n",
    "input_data = tf.random.normal([8,80,100])\n",
    "xt0 = input_data[:,0,:]\n",
    "\n",
    "out,xt1 = simp_cell(xt0,[tf.zeros([8,64])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 64]), TensorShape([8, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape,xt1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'kernel:0' shape=(100, 64) dtype=float32, numpy=\n",
       " array([[-0.10118189,  0.16297732, -0.13973466, ...,  0.18670829,\n",
       "          0.01483352, -0.09996396],\n",
       "        [-0.04824269, -0.11835521, -0.0110748 , ...,  0.01231547,\n",
       "         -0.0790868 , -0.13209271],\n",
       "        [ 0.10127021,  0.00553292,  0.14211215, ..., -0.01101975,\n",
       "          0.14316942,  0.13125478],\n",
       "        ...,\n",
       "        [ 0.02747473, -0.02128161,  0.15769969, ...,  0.16052411,\n",
       "         -0.05295689, -0.18953992],\n",
       "        [-0.05982266,  0.1769282 ,  0.16905452, ...,  0.02833901,\n",
       "         -0.189273  , -0.10281652],\n",
       "        [-0.13563968,  0.10484405, -0.10137802, ...,  0.16961826,\n",
       "         -0.06462876,  0.01164556]], dtype=float32)>,\n",
       " <tf.Variable 'recurrent_kernel:0' shape=(64, 64) dtype=float32, numpy=\n",
       " array([[-0.20373714, -0.06992456,  0.05466264, ...,  0.1458063 ,\n",
       "         -0.17252831, -0.13206734],\n",
       "        [ 0.13179272, -0.1348455 ,  0.19324532, ...,  0.01583637,\n",
       "          0.08473799,  0.16714104],\n",
       "        [ 0.00159304,  0.21257277, -0.08275092, ...,  0.05889675,\n",
       "         -0.14526051, -0.02083752],\n",
       "        ...,\n",
       "        [ 0.0283333 ,  0.01633237, -0.05084246, ...,  0.10775238,\n",
       "         -0.16342045,  0.12216767],\n",
       "        [-0.02044023,  0.14368007,  0.08250934, ..., -0.25133416,\n",
       "         -0.09375305,  0.00986388],\n",
       "        [ 0.01514022,  0.0079961 ,  0.01164089, ..., -0.1006182 ,\n",
       "         -0.02608085,  0.01723556]], dtype=float32)>,\n",
       " <tf.Variable 'bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simp_cell.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多层RNN的实现\n",
    "simp_rnn_cell0 = keras.layers.SimpleRNNCell(64)\n",
    "simp_rnn_cell1 = keras.layers.SimpleRNNCell(64)\n",
    "\n",
    "input_data = tf.random.normal([8,80,100])\n",
    "xt0 = input_data[:,0,:]\n",
    "\n",
    "state0 = [tf.zeros([8,64])]\n",
    "state1 = [tf.zeros([8,64])]\n",
    "\n",
    "out0,state0 = simp_rnn_cell0(xt0,state0)\n",
    "out1,state1 = simp_rnn_cell1(out0,state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对批量的句子进行迭代\n",
    "for words in tf.unstack(input_data,axis=1): # 在1维度上进行迭代\n",
    "    out0,state0 = simp_rnn_cell0(words,state0)\n",
    "    out1,state1 = simp_rnn_cell1(out0,state1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 64 # 这个值代表的是RNN网络cell的输出维度\n",
    "# 一种比较简单的RNN网购狗仔方法:使用Sequential对象进行管理\n",
    "rnn = keras.Sequential([\n",
    "    keras.layers.SimpleRNN(units,dropout = 0.5, return_sequences = True, unroll = True), \n",
    "    # return_sequences 表示每次时间节点的计算都需要范湖一个值给下一层网络当做输入使用, 如果不设置这个值, 那么网络层只返回最后一个时间节点计算得到的值值\n",
    "    keras.layers.SimpleRNN(units,dropout = 0.5, unroll = True)\n",
    "])\n",
    "# SimpleRNN相对于SimpleRNNCell的区别就是, SimpleRNN不需要我们对输入项进行时间维度的抽取切片, 系统会自动完成\n",
    "input_data = tf.random.normal([8,80,100])\n",
    "out = rnn(input_data)\n",
    "# out是最后一层的最优一个时间节点的计算输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
