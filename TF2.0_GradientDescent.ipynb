{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # 使用tf来求解梯度很简单:\n",
    "x = tf.constant(3.)\n",
    "w = tf.constant(5.)\n",
    "y = x*w\n",
    "with tf.GradientTape() as tape:# 这里注意tf.GradientTape后边加括号\n",
    "    tape.watch(w)\n",
    "    # 这里针对所有的静态变量(非Variable的变量),都需要watch, 如果不想watch, 就必须把这个变量定义为Vatiable的类型\n",
    "    # 最好加上watch这一句,针对需要求导的参数都进行观察\n",
    "    # 在这个tape下构建计算图, 并且计算loss\n",
    "# 就可以用[w_grad] = tape.gradient(loss.[w])这样的公式进行计算梯度\n",
    "tape.gradient(y,[w])\n",
    "# 这里因为y和x之间的计算没有被包括在tape之内, 所以梯度求解结果是None\n",
    "# 这个梯度求解方法只能求解一次, 求解之后系统会自动释放一些资源\n",
    "# 为了是tape能够多次调用梯度求解结果,可以在tf.GradientTape()的参数中加上persistent = Ture的参数\n",
    "# 但是这样比较耗费显存, 建议不采用这种办法\n",
    "# 梯度计算代码不应该在tape之内"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=51, shape=(), dtype=float32, numpy=3.0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant(3.)\n",
    "w = tf.constant(5.)\n",
    "with tf.GradientTape() as tape1:\n",
    "    tape1.watch(w)\n",
    "    y2 = w*x\n",
    "tape1.gradient(y2,[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二阶梯度的计算方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3.)\n",
    "w = tf.Variable(5.)\n",
    "b = tf.Variable(4.)\n",
    "with tf.GradientTape() as t1:\n",
    "    with tf.GradientTape() as t2:\n",
    "        yn = w*x +b\n",
    "    d_w,d_b = t2.gradient(yn,[w,b])\n",
    "    print(d_w,d_b)\n",
    "dd_w = t1.gradient(d_w,w)\n",
    "print(dd_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数以及其梯度\n",
    "# sigmoid 梯度计算\n",
    "# grad_s = s*(1-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=176, shape=(10,), dtype=float32, numpy=\n",
       " array([4.5386874e-05, 4.1860685e-04, 3.8362255e-03, 3.3258751e-02,\n",
       "        1.8632649e-01, 1.8632641e-01, 3.3258699e-02, 3.8362255e-03,\n",
       "        4.1854731e-04, 4.5416677e-05], dtype=float32)>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.linspace(-10.,10.,10)\n",
    "with tf.GradientTape() as tape3:\n",
    "    tape3.watch(s)\n",
    "    y = tf.sigmoid(s)\n",
    "tape3.gradient(y,[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh激活函数\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=193, shape=(10,), dtype=float32, numpy=\n",
       " array([0.0000000e+00, 7.1525562e-07, 5.9722963e-05, 5.0775758e-03,\n",
       "        3.5285267e-01, 3.5285220e-01, 5.0775758e-03, 5.9722963e-05,\n",
       "        7.1525562e-07, 0.0000000e+00], dtype=float32)>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = tf.linspace(-10.,10.,10)\n",
    "with tf.GradientTape() as tape4:\n",
    "    tape4.watch(s1)\n",
    "    y1 = tf.tanh(s1)\n",
    "tape4.gradient(y1,[s1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU激活函数,很大程度上解决了梯度弥散和梯度爆炸的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=235, shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>]\n",
      "[<tf.Tensor: id=239, shape=(10,), dtype=float32, numpy=array([0.2, 0.2, 0.2, 0.2, 0.2, 1. , 1. , 1. , 1. , 1. ], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "s2 = tf.linspace(-10.,10.,10)\n",
    "with tf.GradientTape(persistent=True) as tape5:\n",
    "    tape5.watch(s2)\n",
    "    y2 = tf.nn.relu(s2)\n",
    "    y3 = tf.nn.leaky_relu(s2)\n",
    "    # leaky_relu与relu的区别就是在输入值小于0的时候回返回一个很小的非0的梯度\n",
    "print(tape5.gradient(y2,[s2]))\n",
    "print(tape5.gradient(y3,[s2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss的梯度\n",
    "# 1.mean squared error均方差损失函数(mse)\n",
    "# 2.cross entropy error损失函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = tf.random.normal([2,4])\n",
    "w_1 = tf.random.normal([4,3])\n",
    "b_1 = tf.zeros([3])\n",
    "y = tf.constant([2,0])# 用于对比的真实label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=329, shape=(4, 3), dtype=float32, numpy=\n",
       "array([[ 0.10581937, -0.27172017,  0.16590075],\n",
       "       [-0.1837901 ,  0.27098364, -0.08719351],\n",
       "       [-0.11029556,  0.25394598, -0.14365038],\n",
       "       [-0.16678412,  0.21020497, -0.04342084]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape6:\n",
    "    tape6.watch([w_1,b_1])\n",
    "    prob = tf.nn.softmax(i@w_1 + b_1,axis=1)\n",
    "    loss = tf.reduce_sum(tf.losses.MSE(tf.one_hot(y,depth = 3),prob))\n",
    "grads = tape6.gradient(loss,[w_1,b_1])\n",
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=328, shape=(3,), dtype=float32, numpy=array([-0.12350981,  0.24501257, -0.12150272], dtype=float32)>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=509, shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-0.33004153, -0.04946045,  0.379502  ],\n",
       "       [-0.5197544 , -0.24553359,  0.76528794],\n",
       "       [ 0.14473207, -0.17832686,  0.03359476],\n",
       "       [-0.3088742 , -0.09931926,  0.40819344]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算softmax输出的值的交叉熵损失函数的梯度\n",
    "i_1 = tf.random.normal([2,4])\n",
    "w_2 = tf.random.normal([4,3])\n",
    "b_2 = tf.zeros([3])\n",
    "y_1 = tf.constant([2,0])# 用于对比的真实label\n",
    "with tf.GradientTape() as tape7:\n",
    "    tape7.watch([w_2,b_2])\n",
    "    logits = i_1@w_2 + b_2\n",
    "    loss = tf.reduce_sum(tf.losses.categorical_crossentropy(tf.one_hot(y,depth = 3),logits,from_logits = True))\n",
    "    # 这里一步直接包含了softmax的计算, 这样计算相比分开计算softmax和损失函数的, 更加稳定可靠\n",
    "grads = tape7.gradient(loss,[w_2,b_2])\n",
    "grads[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=508, shape=(3,), dtype=float32, numpy=array([-0.25347105,  0.68810594, -0.43463483], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
