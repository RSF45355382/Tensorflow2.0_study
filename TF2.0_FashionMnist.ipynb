{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data,train_label),(test_data,test_label) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape,train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x,y):\n",
    "    x = tf.cast(x,dtype=tf.float32)/255.\n",
    "    y = tf.cast(y,dtype=tf.int32)\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 128\n",
    "\n",
    "db = tf.data.Dataset.from_tensor_slices((train_data,train_label))\n",
    "db = db.map(preprocess).shuffle(10000).batch(batchsize)\n",
    "\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((test_data,test_label))\n",
    "test_db = test_db.map(preprocess).batch(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(256,activation = tf.nn.relu),\n",
    "    keras.layers.Dense(128,activation = tf.nn.relu),\n",
    "    keras.layers.Dense(64,activation = tf.nn.relu),\n",
    "    keras.layers.Dense(32,activation = tf.nn.relu),\n",
    "    keras.layers.Dense(10),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(input_shape=[None,28,28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             multiple                  200960    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             multiple                  32896     \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             multiple                  330       \n",
      "=================================================================\n",
      "Total params: 244,522\n",
      "Trainable params: 244,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # !!!!!!注意, 这里我们使用网络层中不使用softmax, 而是在求解损失函数的时候再单独加上直接从logits计算的选项from_logits = True\n",
    "            # 这样得到的结果更好, 更加收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss_ce:  2.331283 loss_mse:  0.11971407\n",
      "0 100 loss_ce:  0.53743386 loss_mse:  15.88255\n",
      "0 200 loss_ce:  0.41462094 loss_mse:  15.989647\n",
      "0 300 loss_ce:  0.5570139 loss_mse:  20.104881\n",
      "0 400 loss_ce:  0.44796792 loss_mse:  19.97681\n",
      "0 Accuracy:  0.8434\n",
      "1 0 loss_ce:  0.53792655 loss_mse:  19.671814\n",
      "1 100 loss_ce:  0.33905968 loss_mse:  19.170256\n",
      "1 200 loss_ce:  0.4505527 loss_mse:  21.222225\n",
      "1 300 loss_ce:  0.29551846 loss_mse:  23.686775\n",
      "1 400 loss_ce:  0.42928818 loss_mse:  21.344746\n",
      "1 Accuracy:  0.8568\n",
      "2 0 loss_ce:  0.31791908 loss_mse:  20.902279\n",
      "2 100 loss_ce:  0.2940216 loss_mse:  20.540009\n",
      "2 200 loss_ce:  0.30905962 loss_mse:  28.111431\n",
      "2 300 loss_ce:  0.29391754 loss_mse:  26.475342\n",
      "2 400 loss_ce:  0.45076835 loss_mse:  24.39349\n",
      "2 Accuracy:  0.8703\n",
      "3 0 loss_ce:  0.3147937 loss_mse:  24.587744\n",
      "3 100 loss_ce:  0.33674136 loss_mse:  25.551174\n",
      "3 200 loss_ce:  0.17783052 loss_mse:  30.416853\n",
      "3 300 loss_ce:  0.31779572 loss_mse:  29.48692\n",
      "3 400 loss_ce:  0.26135582 loss_mse:  30.359522\n",
      "3 Accuracy:  0.8781\n",
      "4 0 loss_ce:  0.2931698 loss_mse:  31.26051\n",
      "4 100 loss_ce:  0.22829476 loss_mse:  32.471886\n",
      "4 200 loss_ce:  0.21484679 loss_mse:  30.511583\n",
      "4 300 loss_ce:  0.27619764 loss_mse:  40.9083\n",
      "4 400 loss_ce:  0.31472185 loss_mse:  41.550827\n",
      "4 Accuracy:  0.8719\n",
      "5 0 loss_ce:  0.2388404 loss_mse:  39.781372\n",
      "5 100 loss_ce:  0.20921564 loss_mse:  43.477802\n",
      "5 200 loss_ce:  0.2985806 loss_mse:  37.190605\n",
      "5 300 loss_ce:  0.31535244 loss_mse:  42.785\n",
      "5 400 loss_ce:  0.22520715 loss_mse:  38.065464\n",
      "5 Accuracy:  0.8793\n",
      "6 0 loss_ce:  0.22615999 loss_mse:  41.26278\n",
      "6 100 loss_ce:  0.34897596 loss_mse:  41.3499\n",
      "6 200 loss_ce:  0.22932824 loss_mse:  34.232597\n",
      "6 300 loss_ce:  0.2167107 loss_mse:  49.012024\n",
      "6 400 loss_ce:  0.20510328 loss_mse:  46.114704\n",
      "6 Accuracy:  0.8767\n",
      "7 0 loss_ce:  0.26331913 loss_mse:  43.461792\n",
      "7 100 loss_ce:  0.26991934 loss_mse:  50.372543\n",
      "7 200 loss_ce:  0.353253 loss_mse:  49.394238\n",
      "7 300 loss_ce:  0.26589543 loss_mse:  39.73891\n",
      "7 400 loss_ce:  0.2782098 loss_mse:  50.062557\n",
      "7 Accuracy:  0.8846\n",
      "8 0 loss_ce:  0.2794439 loss_mse:  44.832962\n",
      "8 100 loss_ce:  0.1432631 loss_mse:  56.998344\n",
      "8 200 loss_ce:  0.11921457 loss_mse:  63.648594\n",
      "8 300 loss_ce:  0.24133307 loss_mse:  55.831455\n",
      "8 400 loss_ce:  0.18176468 loss_mse:  55.28887\n",
      "8 Accuracy:  0.8846\n",
      "9 0 loss_ce:  0.3311134 loss_mse:  54.507942\n",
      "9 100 loss_ce:  0.3074836 loss_mse:  73.76808\n",
      "9 200 loss_ce:  0.21540217 loss_mse:  55.759583\n",
      "9 300 loss_ce:  0.18205655 loss_mse:  59.31978\n",
      "9 400 loss_ce:  0.1565234 loss_mse:  79.545456\n",
      "9 Accuracy:  0.8859\n",
      "10 0 loss_ce:  0.1441634 loss_mse:  68.00198\n",
      "10 100 loss_ce:  0.1990406 loss_mse:  84.329865\n",
      "10 200 loss_ce:  0.18363424 loss_mse:  75.48588\n",
      "10 300 loss_ce:  0.19828543 loss_mse:  62.017532\n",
      "10 400 loss_ce:  0.17308123 loss_mse:  77.203636\n",
      "10 Accuracy:  0.8851\n",
      "11 0 loss_ce:  0.26630422 loss_mse:  60.283386\n",
      "11 100 loss_ce:  0.3339185 loss_mse:  85.09355\n",
      "11 200 loss_ce:  0.15465918 loss_mse:  70.1687\n",
      "11 300 loss_ce:  0.36834654 loss_mse:  71.992905\n",
      "11 400 loss_ce:  0.15904234 loss_mse:  85.10135\n",
      "11 Accuracy:  0.8854\n",
      "12 0 loss_ce:  0.2890546 loss_mse:  71.35651\n",
      "12 100 loss_ce:  0.21397251 loss_mse:  71.13874\n",
      "12 200 loss_ce:  0.14696696 loss_mse:  89.40599\n",
      "12 300 loss_ce:  0.23400176 loss_mse:  75.41821\n",
      "12 400 loss_ce:  0.22742122 loss_mse:  83.17824\n",
      "12 Accuracy:  0.8838\n",
      "13 0 loss_ce:  0.24805444 loss_mse:  91.63716\n",
      "13 100 loss_ce:  0.14419124 loss_mse:  90.39662\n",
      "13 200 loss_ce:  0.16025272 loss_mse:  77.13828\n",
      "13 300 loss_ce:  0.3830939 loss_mse:  80.489136\n",
      "13 400 loss_ce:  0.17315564 loss_mse:  80.92504\n",
      "13 Accuracy:  0.8891\n",
      "14 0 loss_ce:  0.22648099 loss_mse:  100.70781\n",
      "14 100 loss_ce:  0.25928518 loss_mse:  100.52575\n",
      "14 200 loss_ce:  0.23212269 loss_mse:  107.086334\n",
      "14 300 loss_ce:  0.14999115 loss_mse:  135.29018\n",
      "14 400 loss_ce:  0.23789272 loss_mse:  122.39029\n",
      "14 Accuracy:  0.8867\n",
      "15 0 loss_ce:  0.18175791 loss_mse:  107.8141\n",
      "15 100 loss_ce:  0.20076619 loss_mse:  116.09987\n",
      "15 200 loss_ce:  0.22461702 loss_mse:  125.23085\n",
      "15 300 loss_ce:  0.19282347 loss_mse:  125.532364\n",
      "15 400 loss_ce:  0.14268889 loss_mse:  113.487015\n",
      "15 Accuracy:  0.8836\n",
      "16 0 loss_ce:  0.18123469 loss_mse:  116.772896\n",
      "16 100 loss_ce:  0.16139112 loss_mse:  113.421326\n",
      "16 200 loss_ce:  0.16527376 loss_mse:  112.80061\n",
      "16 300 loss_ce:  0.14078023 loss_mse:  134.41498\n",
      "16 400 loss_ce:  0.22273293 loss_mse:  100.621826\n",
      "16 Accuracy:  0.8871\n",
      "17 0 loss_ce:  0.14872105 loss_mse:  104.7854\n",
      "17 100 loss_ce:  0.20764261 loss_mse:  120.621155\n",
      "17 200 loss_ce:  0.1628185 loss_mse:  124.18537\n",
      "17 300 loss_ce:  0.1400756 loss_mse:  140.09567\n",
      "17 400 loss_ce:  0.14199391 loss_mse:  126.80853\n",
      "17 Accuracy:  0.8942\n",
      "18 0 loss_ce:  0.1124327 loss_mse:  142.08472\n",
      "18 100 loss_ce:  0.12055121 loss_mse:  143.03651\n",
      "18 200 loss_ce:  0.2655178 loss_mse:  101.37126\n",
      "18 300 loss_ce:  0.12681442 loss_mse:  126.43019\n",
      "18 400 loss_ce:  0.16177756 loss_mse:  122.81965\n",
      "18 Accuracy:  0.8848\n",
      "19 0 loss_ce:  0.23694128 loss_mse:  146.18056\n",
      "19 100 loss_ce:  0.1563971 loss_mse:  141.96951\n",
      "19 200 loss_ce:  0.24242485 loss_mse:  125.6683\n",
      "19 300 loss_ce:  0.21299636 loss_mse:  146.25629\n",
      "19 400 loss_ce:  0.1471109 loss_mse:  138.6391\n",
      "19 Accuracy:  0.8901\n",
      "20 0 loss_ce:  0.2852237 loss_mse:  177.02792\n",
      "20 100 loss_ce:  0.14979762 loss_mse:  153.62878\n",
      "20 200 loss_ce:  0.30327833 loss_mse:  143.51517\n",
      "20 300 loss_ce:  0.11755616 loss_mse:  136.42833\n",
      "20 400 loss_ce:  0.1365372 loss_mse:  130.19775\n",
      "20 Accuracy:  0.8892\n",
      "21 0 loss_ce:  0.2732013 loss_mse:  149.11197\n",
      "21 100 loss_ce:  0.15430088 loss_mse:  171.60767\n",
      "21 200 loss_ce:  0.21138161 loss_mse:  153.5873\n",
      "21 300 loss_ce:  0.12726219 loss_mse:  158.22711\n",
      "21 400 loss_ce:  0.2834974 loss_mse:  165.12402\n",
      "21 Accuracy:  0.8876\n",
      "22 0 loss_ce:  0.18022275 loss_mse:  157.59138\n",
      "22 100 loss_ce:  0.18073533 loss_mse:  216.20416\n",
      "22 200 loss_ce:  0.15751493 loss_mse:  161.42015\n",
      "22 300 loss_ce:  0.1522466 loss_mse:  169.85751\n",
      "22 400 loss_ce:  0.13781239 loss_mse:  196.04099\n",
      "22 Accuracy:  0.8884\n",
      "23 0 loss_ce:  0.11264554 loss_mse:  181.16997\n",
      "23 100 loss_ce:  0.15107712 loss_mse:  178.82407\n",
      "23 200 loss_ce:  0.22949028 loss_mse:  192.81595\n",
      "23 300 loss_ce:  0.13241363 loss_mse:  202.44339\n",
      "23 400 loss_ce:  0.11161498 loss_mse:  169.80627\n",
      "23 Accuracy:  0.8894\n",
      "24 0 loss_ce:  0.122488074 loss_mse:  185.15793\n",
      "24 100 loss_ce:  0.2060075 loss_mse:  199.76105\n",
      "24 200 loss_ce:  0.2898021 loss_mse:  183.44559\n",
      "24 300 loss_ce:  0.2336404 loss_mse:  197.86946\n",
      "24 400 loss_ce:  0.17933777 loss_mse:  189.18593\n",
      "24 Accuracy:  0.8955\n",
      "25 0 loss_ce:  0.14113383 loss_mse:  220.37083\n",
      "25 100 loss_ce:  0.197822 loss_mse:  196.45108\n",
      "25 200 loss_ce:  0.17656013 loss_mse:  256.3788\n",
      "25 300 loss_ce:  0.21650146 loss_mse:  186.25627\n",
      "25 400 loss_ce:  0.11001655 loss_mse:  270.6647\n",
      "25 Accuracy:  0.8882\n",
      "26 0 loss_ce:  0.10718046 loss_mse:  270.36966\n",
      "26 100 loss_ce:  0.13019575 loss_mse:  188.25429\n",
      "26 200 loss_ce:  0.11641222 loss_mse:  222.84201\n",
      "26 300 loss_ce:  0.23284224 loss_mse:  220.12271\n",
      "26 400 loss_ce:  0.13359489 loss_mse:  287.72095\n",
      "26 Accuracy:  0.8955\n",
      "27 0 loss_ce:  0.11770729 loss_mse:  226.5629\n",
      "27 100 loss_ce:  0.08825686 loss_mse:  245.04878\n",
      "27 200 loss_ce:  0.16917099 loss_mse:  246.51408\n",
      "27 300 loss_ce:  0.1319465 loss_mse:  198.03137\n",
      "27 400 loss_ce:  0.15671846 loss_mse:  221.94669\n",
      "27 Accuracy:  0.8929\n",
      "28 0 loss_ce:  0.098826095 loss_mse:  196.94148\n",
      "28 100 loss_ce:  0.064749226 loss_mse:  244.09615\n",
      "28 200 loss_ce:  0.08478689 loss_mse:  227.97664\n",
      "28 300 loss_ce:  0.10991033 loss_mse:  229.35884\n",
      "28 400 loss_ce:  0.14668961 loss_mse:  228.46716\n",
      "28 Accuracy:  0.8916\n",
      "29 0 loss_ce:  0.12051788 loss_mse:  212.81372\n",
      "29 100 loss_ce:  0.16508752 loss_mse:  252.77284\n",
      "29 200 loss_ce:  0.22729194 loss_mse:  248.27945\n",
      "29 300 loss_ce:  0.15129517 loss_mse:  230.39688\n",
      "29 400 loss_ce:  0.1299926 loss_mse:  254.80188\n",
      "29 Accuracy:  0.8895\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "for epoch in range(30):\n",
    "    for step,(x,y) in enumerate(db):\n",
    "        y_onehot = tf.one_hot(y,depth = 10)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x)\n",
    "            # !!!!!!注意, 这里我们使用网络层中不适用softmax, 而是在求解损失函数的时候再单独加上直接从logits计算的选项from_logits = True\n",
    "            # 这样得到的结果更好, 更加收敛\n",
    "            loss_mse = tf.reduce_mean(tf.losses.MSE(logits,y_onehot))\n",
    "            loss_ce = tf.reduce_mean(tf.losses.categorical_crossentropy(y_onehot,logits,from_logits = True))\n",
    "        grads = tape.gradient(loss_ce,model.trainable_variables)\n",
    "        # 梯度计算\n",
    "        optimizer.apply_gradients(zip(grads,model.trainable_variables))\n",
    "        # 利用优化器进行梯度更新\n",
    "        if step % 100 == 0:\n",
    "            print(epoch,step,'loss_ce: ',loss_ce.numpy(),'loss_mse: ',loss_mse.numpy())\n",
    "    \n",
    "    # test\n",
    "    correct_total = 0\n",
    "    test_total = 0\n",
    "    for (x,y) in test_db:\n",
    "        pred = tf.argmax(model(x),axis=1)\n",
    "        correct = tf.cast(tf.equal(tf.cast(pred,dtype = tf.int32),y),dtype=tf.int32)\n",
    "        correct_num = tf.reduce_sum(correct)\n",
    "        test_total += x.shape[0]\n",
    "        correct_total += correct_num.numpy()\n",
    "    accuracy = correct_total/test_total\n",
    "    print(epoch,'Accuracy: ',accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "#因为每个样本都是28*28个灰度（2的八次方）图片，其可以用0-255进行标示。\n",
    "#因此单个样本28*28就有768个像素点，为了便于计算，使得计算能够快速收敛，\n",
    "#把用每个像素点的值除以255，让其映射到0-1的区间，这个过程叫归一化（Normalization）。\n",
    "training_images=training_images / 255.0\n",
    "test_images=test_images / 255.0\n",
    "#定义模型，模型是序列的特性的，因此选用Sequential来定义，且具有三个层\n",
    "model = tf.keras.models.Sequential([\n",
    "#第一层，Flatten层，其作用是将每个样本从28*28的格式，按一定的顺序变成一个1-D的含有768个\n",
    "#元素的数组（向量）\n",
    "  tf.keras.layers.Flatten(),\n",
    "#第二层，Dense是全连接层，他具有128个神经元（即输入输出口径有128个，激活函数是relu）\n",
    "#激活函数relu表示从输出到输出的过程是非线性的，而且relu是一种简单粗暴的方式，\n",
    "#其结果就是，当计算值小于0时，强制让其等于0；当计算结果大于0时，就取计算结果。\n",
    "#从某种程度上讲提高了计算速度，保证了稀疏性。\n",
    "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "#第三层，Dense是全连接层，他具有10个神经元，因为有10类的label,其激活函数softmax的\n",
    "#作用就是计算每个样本数据data与属于这个10个label的概率，结果去概率值最大的label,作为最终结果。\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "#配置模型，使用adam优化，损失函数是sparse_categorical_crossentropy，使用精确性来度\n",
    "#量模型优劣\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#开始训练，给模型喂训练数据集（training_images, training_labels）\n",
    "#epochs=5, 意思是用这个训练集将模型训练5遍。\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "#使用测试集数据来验证模型，查看模型的好坏\n",
    "test_loss = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
