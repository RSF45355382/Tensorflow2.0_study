{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先介绍mse误差, mean square error, 均方差误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.constant([1,2,3,0,2])\n",
    "y = tf.one_hot(y,depth=4)\n",
    "y = tf.cast(y,tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tf.random.normal([5,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.9819967, shape=(), dtype=float32)\n",
      "tf.Tensor(0.98199666, shape=(), dtype=float32)\n",
      "tf.Tensor(0.9819967, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.reduce_mean(tf.square(y-out)))\n",
    "# 直接公式计算mse误差\n",
    "print(tf.square(tf.norm(y-out))/(5*4))\n",
    "# 使用范数的形式来计算mse\n",
    "print(tf.reduce_mean(tf.losses.MSE(y,out)))\n",
    "# 直接调用接口计算mse,tf.losses.MSE(y,out)) 返回的是没一个样本的mse ,求总体的mse 还要进一步进行求平均操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其次我们介绍交叉熵误差\n",
    "# 熵代表的是一个信息量的概念, 熵越小, 代表信息量越多, 就越不稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.fill([4],0.25)\n",
    "# 构造一个长度为4, 没一个元素都是0.25的tensor,也就是每一种情况的概率相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=76, shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算交叉熵,一般计算交叉熵用的log是已2为底数的, 但是tf中的log函数是以e为底数的, 所以需要额外运算转换\n",
    "-tf.reduce_sum(c*tf.math.log(c)/tf.math.log(2.0))\n",
    "# 公式法计算交叉熵,结果得到是2, 这是一个比较大的熵值,代表概率均等, 没什么更多信息, 惊喜度很低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=85, shape=(), dtype=float32, numpy=1.3567796>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf.constant([0.1,0.1,0.1,0.7])\n",
    "-tf.reduce_sum(d*tf.math.log(d)/tf.math.log(2.0))\n",
    "# 概率分布不均等, 获得的交叉熵较小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=94, shape=(), dtype=float32, numpy=0.03421897>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tf.constant([0.001,0.001,0.001,0.997])\n",
    "-tf.reduce_sum(e*tf.math.log(e)/tf.math.log(2.0))\n",
    "# 概率分布更不均等, 获得的交叉熵更小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交叉熵代表的是两个概率分布之间的差别大小的一个评估\n",
    "# 详细的交叉熵计算公式查书本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=111, shape=(), dtype=float32, numpy=1.3862944>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多分类情况下的交叉熵计算代码\n",
    "tf.losses.categorical_crossentropy([0,0,1,0],[0.25,0.25,0.25,0.25])\n",
    "# 第一个参数是真实分布, 第二个参数是获取的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=128, shape=(), dtype=float32, numpy=0.030459179>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.categorical_crossentropy([0,0,1,0],[0.01,0.01,0.97,0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=145, shape=(), dtype=float32, numpy=4.6051702>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.categorical_crossentropy([0,0,1,0],[0.01,0.01,0.01,0.97])\n",
    "# 预测错误情况下的交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=180, shape=(), dtype=float32, numpy=2.3025842>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 二分类情况下的交叉熵\n",
    "# 两种计算方法, 第一个是通过类进行计算, 另一种是通过方法来进行计算\n",
    "tf.losses.BinaryCrossentropy()([1],[0.1]) # 注意,这里会多一个括号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=205, shape=(), dtype=float32, numpy=2.3025842>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.binary_crossentropy([1],[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=230, shape=(), dtype=float32, numpy=0.10536041>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.losses.binary_crossentropy([0],[0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在我们工程应用中, 可以将softmax层和交叉熵loss计算层合并成一层\n",
    "# tf.losses.categorical_crossentropy(one_hot_label, output, from_logits=True) # 注意这里output是还没有被softmax处理过的输出\n",
    "# 而普通的计算交叉熵的方式是:\n",
    "# tf.losses.categorical_crossentropy(one_hot_label, prob)# 这里的prob是output经过softmax处理之后的概率分布\n",
    "# 一般我们推荐第一种方式\n",
    "# 这样就得到的最终的交叉熵的值,更加稳定, 更加不易出现错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
